# -*- coding: utf-8 -*-
"""diabetes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c4jc4FW5zQdDhzzf_N8btw8dC_yWzdQ0
"""

# Commented out IPython magic to ensure Python compatibility.
# importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import KFold
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from mlxtend.plotting import plot_confusion_matrix

from sklearn.ensemble import RandomForestClassifier
import xgboost

data= pd.read_csv("diabetes.csv")

print("shape of dataset :", data.shape)
data.head()

data.info()

"""### checking missing values"""

data.isnull().sum()

"""*There are no missing values*

### Checking class imbalance
"""

data["Outcome"].value_counts()/len(data)*100

sns.countplot(data["Outcome"])

"""*The class distribution in the `Target` variable is ~65:35 which is not exactly imbalanced.*

### Feature Selection
"""

plt.figure(figsize=(13,8))
sns.heatmap(data.corr(), annot=True)

data.describe()

"""*As it can be seen that columns `Pregnancies` `Glucose` `BloodPressure` `SkinThickness` `Insulin` `BMI` has 0 values . Therefore these columns need to be imputed with their respective mean value.*

### Separating dependent and independent variables
"""

# # feature variables
# x= data.iloc[:, :-1]

# # target variable
# y= data.iloc[:, -1]

x = data.drop('Outcome', axis = 1)
y = data['Outcome']

"""### Splitting data into *train* and *test*"""

x_train, x_test, y_train, y_test= train_test_split(x,y, test_size=0.2, random_state=1)

"""### Imputing with mean"""

from sklearn.impute import SimpleImputer
impute= SimpleImputer(missing_values= 0,  strategy='mean')
x_train= impute.fit_transform(x_train)
x_test= impute.fit_transform(x_test)

"""## Model Building

### Random Forest
"""

rf_classifier= RandomForestClassifier()

rf_params= {"n_estimators" : [int(x) for x in np.linspace( start=100, stop=1000, num=10)],
            "criterion" : ["gini", "entropy"],
            "max_depth" : [int(x) for x in np.linspace(start= 100, stop=1000, num=10)],
            # "min_samples_split": [1,3,6,7,8],
            "min_samples_split": [2, 3, 6, 7, 8],
            "min_samples_leaf" : [3,4,6,7,9],
            "max_features" : ["auto", "sqrt", "log2"]}

rf_cv= KFold(n_splits=5)

rf_randomSearch= RandomizedSearchCV(estimator= rf_classifier,param_distributions=rf_params, n_iter=100,
                                    scoring="f1_macro", n_jobs=-1, cv= rf_cv, verbose=2, random_state=10)

rf_randomSearch.fit(x_train, y_train)

print(rf_randomSearch.best_score_)
print(rf_randomSearch.best_params_)
print(rf_randomSearch.best_estimator_)

# best model
classifier_rf= rf_randomSearch.best_estimator_

rf_pred= classifier_rf.predict(x_test)

cm= confusion_matrix(y_test, rf_pred)
plot_confusion_matrix(cm, figsize=(6,6))

print(accuracy_score(y_test, rf_pred))
print(classification_report(y_test, rf_pred))

"""### XgBoost"""

xgboost_classifier= xgboost.XGBClassifier()

xgboost_params= {"learning_rate": [0.1, 0.2, 0.25, 0.3, 0.35],
                 "max_depth": [3,4,5,7,8,9],
                 "min_child_weight": [2,4,5,6],
                 "gamma": [0.1, 0.2, 0.3, 0.4],
                 "colsample_bytree": [0.2,0.3, 0.4, 0.5]}

xgboost_cv= KFold(n_splits=5)

xgboost_search= RandomizedSearchCV(estimator= xgboost_classifier, param_distributions=xgboost_params, n_iter=100,
                                   scoring= "roc_auc", n_jobs=-1, cv=xgboost_cv, verbose=2, random_state=1)


xgboost_search.fit(x_train, y_train)

print(xgboost_search.best_score_)
print(xgboost_search.best_params_)
print(xgboost_search.best_estimator_)

classifier_xgboost= xgboost_search.best_estimator_

xgboost_pred= classifier_xgboost.predict(x_test)

cm2= confusion_matrix(y_test, xgboost_pred)
plot_confusion_matrix(cm2, figsize=(6,6))

print(accuracy_score(y_test, xgboost_pred))
print(classification_report(y_test, xgboost_pred))